import numpy as np
import open3d as o3d
import laspy
import os
import json
import time
import argparse
from sklearn.linear_model import RANSACRegressor, LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt
from tqdm import tqdm

# æ‹Ÿåˆç”µåŠ›çº¿ï¼Œè¿”å›å½’ä¸€åŒ–jsonï¼ŒhtmlæŠ¥å‘Šï¼ŒtxtæŠ¥å‘Š
# è°ƒç”¨å½’ä¸€åŒ–å‚æ•°jsonæ–‡ä»¶ï¼špython 4-ransac_fit_to_json.py -i D:\programs\è„šæœ¬æ…¢æ…¢æ¥\powerlines\tile_456000_3120000\all_power_lines.las -o D:\programs\è„šæœ¬æ…¢æ…¢æ¥\powerlines_line_json -nf D:\programs\è„šæœ¬æ…¢æ…¢æ¥\tileåˆ†å—\tile_threejs_params.json
# å¯è§†åŒ–ç»“æœäº§çœ‹ï¼špython 4-ransac_fit_to_json.py --input D:\programs\lasæ•°æ®æå–\lasæ•°æ®\éº»å«é£\all_power_lines.las --output_dir D:\programs\è„šæœ¬æ…¢æ…¢æ¥\powerlines_line_json --visualize

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


def get_available_filename(base_path):
    """
    ç”Ÿæˆä¸€ä¸ªå¯ç”¨çš„æ–‡ä»¶åï¼Œå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™æ·»åŠ åºå·
    
    å‚æ•°:
        base_path: åŸºæœ¬æ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        å¯ç”¨çš„æ–‡ä»¶è·¯å¾„
    """
    if not os.path.exists(base_path):
        return base_path
    
    # åˆ†è§£æ–‡ä»¶è·¯å¾„
    directory = os.path.dirname(base_path)
    filename = os.path.basename(base_path)
    name, ext = os.path.splitext(filename)
    
    # å°è¯•æ·»åŠ åºå·
    counter = 1
    while True:
        new_path = os.path.join(directory, f"{name}-{counter}{ext}")
        if not os.path.exists(new_path):
            return new_path
        counter += 1


def load_normalization_params(normalization_file):
    """
    åŠ è½½å½’ä¸€åŒ–å‚æ•°JSONæ–‡ä»¶
    
    å‚æ•°:
        normalization_file: å½’ä¸€åŒ–å‚æ•°JSONæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        dict: å½’ä¸€åŒ–å‚æ•°ï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å›None
    """
    try:
        with open(normalization_file, 'r', encoding='utf-8') as f:
            params = json.load(f)
        
        # éªŒè¯å¿…è¦çš„å‚æ•°æ˜¯å¦å­˜åœ¨
        required_keys = ['center', 'normalization']
        if not all(key in params for key in required_keys):
            print(f"è­¦å‘Š: å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶ç¼ºå°‘å¿…è¦çš„é”®: {required_keys}")
            return None
        
        # éªŒè¯å½’ä¸€åŒ–å‚æ•°ç»“æ„
        norm_params = params['normalization']
        if 'scale' not in norm_params or 'translate' not in norm_params:
            print("è­¦å‘Š: å½’ä¸€åŒ–å‚æ•°ç»“æ„ä¸æ­£ç¡®")
            return None
        
        translate = norm_params['translate']
        if not all(key in translate for key in ['x', 'y', 'z']):
            print("è­¦å‘Š: å¹³ç§»å‚æ•°ç¼ºå°‘x,y,zåæ ‡")
            return None
        
        print(f"æˆåŠŸåŠ è½½å½’ä¸€åŒ–å‚æ•°:")
        print(f"  æ•°æ®ä¸­å¿ƒ: ({params['center']['x']:.2f}, {params['center']['y']:.2f}, {params['center']['z']:.2f})")
        print(f"  ç¼©æ”¾å› å­: {norm_params['scale']:.6f}")
        print(f"  å¹³ç§»é‡: ({translate['x']:.2f}, {translate['y']:.2f}, {translate['z']:.2f})")
        
        return params
        
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶: {normalization_file}")
        return None
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶JSONæ ¼å¼é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½å½’ä¸€åŒ–å‚æ•°æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return None


def apply_normalization(points, normalization_params):
    """
    å¯¹ç‚¹äº‘åº”ç”¨å½’ä¸€åŒ–å˜æ¢
    
    å‚æ•°:
        points: åŸå§‹ç‚¹äº‘ numpyæ•°ç»„ (N, 3)
        normalization_params: å½’ä¸€åŒ–å‚æ•°å­—å…¸
        
    è¿”å›:
        numpyæ•°ç»„: å½’ä¸€åŒ–åçš„ç‚¹äº‘
    """
    if normalization_params is None:
        print("è­¦å‘Š: å½’ä¸€åŒ–å‚æ•°ä¸ºç©ºï¼Œè¿”å›åŸå§‹åæ ‡")
        return points
    
    try:
        # è·å–å½’ä¸€åŒ–å‚æ•°
        scale = normalization_params['normalization']['scale']
        translate = normalization_params['normalization']['translate']
        
        # åˆ›å»ºå¹³ç§»å‘é‡
        translate_vector = np.array([translate['x'], translate['y'], translate['z']])
        
        # åº”ç”¨å½’ä¸€åŒ–: (åŸå§‹åæ ‡ + å¹³ç§») * ç¼©æ”¾
        normalized_points = (points + translate_vector) * scale
        
        return normalized_points
        
    except Exception as e:
        print(f"è­¦å‘Š: åº”ç”¨å½’ä¸€åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {e}ï¼Œè¿”å›åŸå§‹åæ ‡")
        return points


class RANSACPowerLineFitting:
    """
    åŸºäºRANSACçš„ç”µåŠ›çº¿é‡å»º - ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ”¯æŒå½’ä¸€åŒ–è¾“å‡º
    """
    
    def __init__(self, 
                 segment_length=50.0,        # åˆ†æ®µé•¿åº¦
                 overlap_ratio=0.2,          # é‡å æ¯”ä¾‹
                 ransac_threshold=1.5,       # RANSACé˜ˆå€¼(æ”¾å®½)
                 min_samples_ratio=0.5,      # æœ€å°æ ·æœ¬æ¯”ä¾‹(é™ä½)
                 polynomial_degree=1,        # çº¿æ€§æ‹Ÿåˆ(ç®€åŒ–)
                 normalization_file=None):   # å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶
        """
        å‚æ•°:
        - segment_length: æ¯æ®µçš„é•¿åº¦(ç±³)
        - overlap_ratio: ç›¸é‚»æ®µçš„é‡å æ¯”ä¾‹
        - ransac_threshold: RANSACå†…ç‚¹é˜ˆå€¼(ç±³) - æ”¾å®½åˆ°1.5ç±³
        - min_samples_ratio: RANSACæœ€å°æ ·æœ¬æ¯”ä¾‹ - é™ä½åˆ°50%
        - polynomial_degree: 1=çº¿æ€§æ‹Ÿåˆ(ç®€åŒ–)
        - normalization_file: å½’ä¸€åŒ–å‚æ•°JSONæ–‡ä»¶è·¯å¾„
        """
        self.segment_length = segment_length
        self.overlap_ratio = overlap_ratio
        self.ransac_threshold = ransac_threshold
        self.min_samples_ratio = min_samples_ratio
        self.polynomial_degree = polynomial_degree
        
        # åŠ è½½å½’ä¸€åŒ–å‚æ•°
        self.normalization_params = None
        if normalization_file:
            self.normalization_params = load_normalization_params(normalization_file)
            if self.normalization_params:
                print("å°†ä½¿ç”¨å½’ä¸€åŒ–åæ ‡è¾“å‡ºç»“æœ")
            else:
                print("å°†ä½¿ç”¨åŸå§‹åæ ‡è¾“å‡ºç»“æœ")
        else:
            print("æœªæŒ‡å®šå½’ä¸€åŒ–å‚æ•°æ–‡ä»¶ï¼Œå°†ä½¿ç”¨åŸå§‹åæ ‡")
    
    def load_point_cloud(self, file_path):
        """åŠ è½½ç‚¹äº‘æ–‡ä»¶"""
        print(f"åŠ è½½ç‚¹äº‘æ–‡ä»¶: {file_path}")
        
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.las' or ext == '.laz':
            las = laspy.read(file_path)
            points = np.vstack((las.x, las.y, las.z)).transpose()
            
            # è¯»å–åˆ†ç±»ä¿¡æ¯ï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„ç”µåŠ›çº¿
            if hasattr(las, 'classification'):
                classifications = las.classification
            else:
                classifications = np.ones(len(points), dtype=np.uint8)
            
            print(f"è¯»å–LASæ–‡ä»¶: {len(points)}ä¸ªç‚¹")
            return points, classifications
        else:
            raise ValueError(f"åªæ”¯æŒLAS/LAZæ–‡ä»¶æ ¼å¼")
    
    def extract_powerline_segments(self, points, classifications):
        """
        é«˜æ•ˆæå–ç”µåŠ›çº¿æ®µ - é¿å…å†…å­˜çˆ†ç‚¸
        è€ƒè™‘åˆ†ç±»ä¿¡æ¯ï¼Œå°†ä¸åŒåˆ†ç±»çš„ç‚¹åˆ†ä¸ºä¸åŒçš„ç”µåŠ›çº¿
        """
        print("å¼€å§‹æå–ç”µåŠ›çº¿æ®µ...")
        
        # ä½¿ç”¨åˆ†ç±»ä¿¡æ¯åˆ†ç»„
        unique_classes = np.unique(classifications)
        print(f"å‘ç° {len(unique_classes)} ä¸ªä¸åŒçš„ç”µåŠ›çº¿åˆ†ç±»")
        
        segments = []
        
        for cls in unique_classes:
            # è·³è¿‡0åˆ†ç±»ï¼ˆé€šå¸¸æ˜¯å™ªå£°ç‚¹ï¼‰
            if cls == 0:
                continue
                
            # æå–å½“å‰åˆ†ç±»çš„ç‚¹
            class_points = points[classifications == cls]
            
            if len(class_points) < 10:
                print(f"  åˆ†ç±» {cls} ç‚¹æ•°å¤ªå°‘ï¼Œè·³è¿‡ ({len(class_points)}ç‚¹)")
                continue
                
            print(f"  å¤„ç†åˆ†ç±» {cls}: {len(class_points)}ç‚¹")
            
            # å¯¹äºç‚¹æ•°å¾ˆå°‘çš„æƒ…å†µï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªæ®µå¤„ç†
            if len(class_points) < 20:
                print(f"    ç‚¹æ•°å°‘äº20ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªæ®µå¤„ç†")
                segments.append({
                    'points': class_points,
                    'classification': cls,
                    'segment_id': len(segments) + 1
                })
                continue
            
            # ä½¿ç”¨DBSCANè¿›ä¸€æ­¥ç»†åˆ†æ¯ä¸ªåˆ†ç±»ä¸­çš„çº¿æ®µ
            # è®¡ç®—è‡ªé€‚åº”epså€¼
            try:
                k = min(10, len(class_points) - 1)  # è‡ªé€‚åº”kå€¼ï¼Œç¡®ä¿ä¸è¶…è¿‡ç‚¹æ•°
                nbrs = NearestNeighbors(n_neighbors=k+1).fit(class_points)  # +1å› ä¸ºåŒ…æ‹¬ç‚¹è‡ªèº«
                distances, indices = nbrs.kneighbors(class_points)
                
                # ä½¿ç”¨ç¬¬kä¸ªè¿‘é‚»çš„å¹³å‡è·ç¦»
                avg_kth_distance = np.mean(distances[:, -1])
                eps = avg_kth_distance * 2.5  # è°ƒæ•´ç³»æ•°
                
                # ç¡®ä¿epsä¸ä¸º0ä¸”ä¸å¤ªå°
                if eps < 0.01:
                    eps = 0.01
                    print(f"    èšç±»åŠå¾„å¤ªå°ï¼Œè®¾ç½®ä¸ºæœ€å°å€¼: {eps:.2f}ç±³")
                else:
                    print(f"    è‡ªé€‚åº”èšç±»åŠå¾„: {eps:.2f}ç±³ (åŸºäº{k}è¿‘é‚»)")
                
                # å¦‚æœepså¤ªå¤§ï¼Œå¼ºåˆ¶é™åˆ¶
                if eps > 20.0:
                    eps = 15.0
                    print(f"    é™åˆ¶èšç±»åŠå¾„ä¸º: {eps:.2f}ç±³")
                
                # DBSCANèšç±»
                clustering = DBSCAN(eps=eps, min_samples=min(5, len(class_points) // 2)).fit(class_points)
                labels = clustering.labels_
                
                # æå–èšç±»
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                
                print(f"    èšç±»ç»“æœ: {n_clusters}ä¸ªèšç±», {n_noise}ä¸ªå™ªå£°ç‚¹")
                
                if n_clusters > 0:
                    for i in range(n_clusters):
                        cluster_points = class_points[labels == i]
                        if len(cluster_points) >= min(8, len(class_points) // 2):  # ç¡®ä¿å­èšç±»æœ‰è¶³å¤Ÿçš„ç‚¹
                            segments.append({
                                'points': cluster_points,
                                'classification': cls,
                                'segment_id': len(segments) + 1
                            })
                else:
                    # å¦‚æœèšç±»å¤±è´¥ï¼Œå°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªæ®µ
                    print(f"    èšç±»å¤±è´¥ï¼Œå°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªæ®µ")
                    segments.append({
                        'points': class_points,
                        'classification': cls,
                        'segment_id': len(segments) + 1
                    })
            except Exception as e:
                print(f"    èšç±»å‡ºé”™: {e}ï¼Œå°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªæ®µ")
                segments.append({
                    'points': class_points,
                    'classification': cls,
                    'segment_id': len(segments) + 1
                })
        
        print(f"æå–äº† {len(segments)} ä¸ªæœ‰æ•ˆç”µåŠ›çº¿æ®µ")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ®µï¼Œå°è¯•å¿½ç•¥åˆ†ç±»ä¿¡æ¯ç›´æ¥èšç±»
        if len(segments) == 0:
            print("  æœªæ‰¾åˆ°æœ‰æ•ˆæ®µï¼Œå°è¯•å¿½ç•¥åˆ†ç±»ä¿¡æ¯...")
            
            try:
                # å¯¹äºç‚¹æ•°å¾ˆå°‘çš„æƒ…å†µï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªæ®µå¤„ç†
                if len(points) < 20:
                    print(f"    æ€»ç‚¹æ•°å°‘äº20ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªæ®µå¤„ç†")
                    segments.append({
                        'points': points,
                        'classification': 1,  # é»˜è®¤åˆ†ç±»
                        'segment_id': 1
                    })
                    return segments
                
                # è®¡ç®—è‡ªé€‚åº”epså€¼
                k = min(10, len(points) - 1)
                nbrs = NearestNeighbors(n_neighbors=k+1).fit(points)
                distances, indices = nbrs.kneighbors(points)
                avg_kth_distance = np.mean(distances[:, -1])
                eps = max(0.01, avg_kth_distance * 2.5)  # ç¡®ä¿epsä¸ä¸º0
                
                # DBSCANèšç±»
                clustering = DBSCAN(eps=eps, min_samples=min(5, len(points) // 2)).fit(points)
                labels = clustering.labels_
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                
                if n_clusters > 0:
                    for i in range(n_clusters):
                        cluster_points = points[labels == i]
                        if len(cluster_points) >= min(8, len(points) // 2):
                            segments.append({
                                'points': cluster_points,
                                'classification': 1,  # é»˜è®¤åˆ†ç±»
                                'segment_id': len(segments) + 1
                            })
                else:
                    # å¦‚æœèšç±»å¤±è´¥ï¼Œå°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªæ®µ
                    segments.append({
                        'points': points,
                        'classification': 1,
                        'segment_id': 1
                    })
            except Exception as e:
                print(f"  å¿½ç•¥åˆ†ç±»èšç±»ä¹Ÿå¤±è´¥: {e}ï¼Œå°†æ‰€æœ‰ç‚¹ä½œä¸ºä¸€ä¸ªæ®µ")
                segments.append({
                    'points': points,
                    'classification': 1,
                    'segment_id': 1
                })
            
            print(f"  å¿½ç•¥åˆ†ç±»å: {len(segments)} ä¸ªç”µåŠ›çº¿æ®µ")
        
        return segments
    
    def fit_powerline_segment(self, segment_data):
        """
        æ‹Ÿåˆå•ä¸ªç”µåŠ›çº¿æ®µ
        """
        segment_points = segment_data['points']
        classification = segment_data['classification']
        segment_id = segment_data['segment_id']
        
        if len(segment_points) < 5:
            print(f"  æ®µ {segment_id} ç‚¹æ•°å¤ªå°‘({len(segment_points)}ç‚¹)ï¼Œæ— æ³•æ‹Ÿåˆ")
            return None, {
                'total_points': len(segment_points),
                'fitted_segments': 0,
                'total_length': 0,
                'classification': classification,
                'segment_id': segment_id
            }
        
        try:
            # PCAç¡®å®šä¸»æ–¹å‘
            pca = PCA(n_components=3)
            pca.fit(segment_points)
            
            # æŠ•å½±åˆ°ä¸»æ–¹å‘
            main_direction = pca.components_[0]
            projected_distances = segment_points.dot(main_direction)
            
            # æŒ‰ä¸»æ–¹å‘æ’åº
            sorted_indices = np.argsort(projected_distances)
            sorted_points = segment_points[sorted_indices]
            
            # è®¡ç®—æ²¿ä¸»æ–¹å‘çš„è·ç¦»
            distances_1d = projected_distances[sorted_indices]
            distances_1d = distances_1d - distances_1d[0]  # ä»0å¼€å§‹
            
            # è·å–æ€»é•¿åº¦
            total_length = distances_1d[-1] if len(distances_1d) > 0 else 0
            
            # åˆ†æ®µæ‹Ÿåˆ
            segments_fitted = []
            
            # å¯¹äºå¾ˆçŸ­çš„çº¿æ®µæˆ–ç‚¹æ•°å¾ˆå°‘çš„æƒ…å†µï¼Œä¸è¿›è¡Œåˆ†æ®µ
            if total_length < self.segment_length or len(segment_points) < 20:
                # çŸ­çº¿æ®µï¼Œç›´æ¥æ‹Ÿåˆ
                curve = self.ransac_fit_segment(sorted_points, distances_1d)
                if curve is not None:
                    curve['classification'] = classification
                    curve['segment_id'] = segment_id
                    curve['subsegment_id'] = 1
                    segments_fitted.append(curve)
            else:
                # é•¿çº¿æ®µï¼Œåˆ†æ®µæ‹Ÿåˆ
                n_segments = int(np.ceil(total_length / self.segment_length))
                step = total_length / n_segments
                
                for i in range(n_segments):
                    start_dist = i * step * (1 - self.overlap_ratio)
                    end_dist = (i + 1) * step
                    
                    # é€‰æ‹©å½“å‰æ®µçš„ç‚¹
                    mask = (distances_1d >= start_dist) & (distances_1d <= end_dist)
                    if np.sum(mask) < 5:
                        continue
                    
                    segment_subset = sorted_points[mask]
                    segment_distances = distances_1d[mask]
                    
                    curve = self.ransac_fit_segment(segment_subset, segment_distances)
                    if curve is not None:
                        curve['classification'] = classification
                        curve['segment_id'] = segment_id
                        curve['subsegment_id'] = i + 1
                        segments_fitted.append(curve)
            
            return segments_fitted, {
                'total_points': len(segment_points),
                'fitted_segments': len(segments_fitted),
                'total_length': total_length,
                'classification': classification,
                'segment_id': segment_id
            }
        
        except Exception as e:
            print(f"  æ®µ {segment_id} æ‹Ÿåˆå‡ºé”™: {e}")
            return None, {
                'total_points': len(segment_points),
                'fitted_segments': 0,
                'total_length': 0,
                'classification': classification,
                'segment_id': segment_id
            }
    
    def ransac_fit_segment(self, points, distances_1d):
        """
        ä½¿ç”¨RANSACæ‹Ÿåˆå•ä¸ªæ®µ - ç®€åŒ–ç‰ˆé¿å…å¤æ‚ä¾èµ–
        """
        if len(points) < 5:
            return None
        
        try:
            # å‡†å¤‡æ•°æ®
            X = distances_1d.reshape(-1, 1)
            
            # ç®€åŒ–ï¼šä½¿ç”¨çº¿æ€§æ‹Ÿåˆä»£æ›¿å¤šé¡¹å¼æ‹Ÿåˆ
            # åˆ†åˆ«æ‹Ÿåˆx, y, zåæ ‡
            fitted_coords = []
            inlier_masks = []
            
            for coord_idx in range(3):
                y = points[:, coord_idx]
                
                # ç¡®ä¿æœ€å°æ ·æœ¬æ•°ä¸è¶…è¿‡æ€»æ ·æœ¬æ•°
                min_samples = max(3, min(len(points) - 1, int(len(points) * self.min_samples_ratio)))
                
                # RANSACçº¿æ€§æ‹Ÿåˆ
                ransac = RANSACRegressor(
                    estimator=LinearRegression(),
                    min_samples=min_samples,
                    residual_threshold=self.ransac_threshold,
                    max_trials=50,  # å‡å°‘å°è¯•æ¬¡æ•°
                    random_state=42
                )
                
                ransac.fit(X, y)
                fitted_coords.append(ransac)
                inlier_masks.append(ransac.inlier_mask_)
            
            # ç”Ÿæˆæ‹Ÿåˆæ›²çº¿
            num_points = min(50, max(2, len(points)))  # ç¡®ä¿è‡³å°‘æœ‰2ä¸ªç‚¹ï¼Œæœ€å¤š50ä¸ªç‚¹
            curve_distances = np.linspace(distances_1d.min(), distances_1d.max(), num_points)
            curve_points = np.zeros((num_points, 3))
            
            X_curve = curve_distances.reshape(-1, 1)
            
            for coord_idx in range(3):
                curve_points[:, coord_idx] = fitted_coords[coord_idx].predict(X_curve)
            
            # è®¡ç®—æ•´ä½“å†…ç‚¹æ©ç ï¼ˆä¸‰ä¸ªåæ ‡çš„äº¤é›†ï¼‰
            overall_inlier_mask = inlier_masks[0] & inlier_masks[1] & inlier_masks[2]
            
            # è®¡ç®—æ‹Ÿåˆå‚æ•°ï¼ˆç›´çº¿æ–¹ç¨‹çš„ç³»æ•°ï¼‰
            coefs_x = fitted_coords[0].estimator_.coef_[0], fitted_coords[0].estimator_.intercept_
            coefs_y = fitted_coords[1].estimator_.coef_[0], fitted_coords[1].estimator_.intercept_
            coefs_z = fitted_coords[2].estimator_.coef_[0], fitted_coords[2].estimator_.intercept_
            
            return {
                'points': curve_points,
                'inlier_mask': overall_inlier_mask,
                'inlier_ratio': np.sum(overall_inlier_mask) / len(points),
                'rmse': np.mean([
                    np.sqrt(np.mean((points[:, i] - fitted_coords[i].predict(X))**2)) 
                    for i in range(3)
                ]),
                'coefs': {
                    'x': {'slope': float(coefs_x[0]), 'intercept': float(coefs_x[1])},
                    'y': {'slope': float(coefs_y[0]), 'intercept': float(coefs_y[1])},
                    'z': {'slope': float(coefs_z[0]), 'intercept': float(coefs_z[1])}
                },
                'start_point': curve_points[0].tolist(),
                'end_point': curve_points[-1].tolist(),
                'original_points_count': len(points)  # æ·»åŠ åŸå§‹ç‚¹æ•°
            }
            
        except Exception as e:
            print(f"RANSACæ‹Ÿåˆå¤±è´¥: {e}")
            return None
    
    def process_powerline_cloud(self, file_path, output_dir=None, save_json=True, visualize=False, save_xyz=False, save_html=True, auto_increment=True):
        """
        å¤„ç†ç”µåŠ›çº¿ç‚¹äº‘çš„ä¸»å‡½æ•°
        
        å‚æ•°:
            file_path: è¾“å…¥LASæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            save_json: æ˜¯å¦ä¿å­˜JSONæ–‡ä»¶
            visualize: æ˜¯å¦æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ
            save_xyz: æ˜¯å¦ä¿å­˜XYZæ–‡ä»¶
            save_html: æ˜¯å¦ä¿å­˜HTMLæŠ¥å‘Š
            auto_increment: æ˜¯å¦è‡ªåŠ¨ä¸ºè¾“å‡ºæ–‡ä»¶æ·»åŠ åºå·
        """
        print("="*80)
        print("å¼€å§‹åŸºäºRANSACçš„ç”µåŠ›çº¿é‡å»º")
        print("="*80)
        
        start_process_time = time.time()
        
        # 1. åŠ è½½ç‚¹äº‘
        points, classifications = self.load_point_cloud(file_path)
        
        # 2. æå–ç”µåŠ›çº¿æ®µ
        powerline_segments = self.extract_powerline_segments(points, classifications)
        
        # 3. æ‹Ÿåˆæ¯ä¸ªæ®µ
        all_fitted_curves = []
        all_fit_infos = []
        
        start_fit_time = time.time()
        
        for i, segment in enumerate(powerline_segments):
            print(f"\nå¤„ç†ç”µåŠ›çº¿æ®µ {i+1}/{len(powerline_segments)} (ç‚¹æ•°: {len(segment['points'])})")
            
            fitted_curves, fit_info = self.fit_powerline_segment(segment)
            
            if fitted_curves:
                all_fitted_curves.extend(fitted_curves)
                all_fit_infos.append(fit_info)
                print(f"  æˆåŠŸæ‹Ÿåˆ {len(fitted_curves)} ä¸ªå­æ®µ")
            else:
                all_fit_infos.append(fit_info)
                print(f"  æ‹Ÿåˆå¤±è´¥")
        
        fit_time = time.time() - start_fit_time
        total_time = time.time() - start_process_time
        
        # 4. ç»“æœç»Ÿè®¡
        print(f"\n{'='*80}")
        print("RANSACé‡å»ºå®Œæˆ - ç»“æœç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"åŸå§‹ç‚¹æ•°: {len(points):,}")
        print(f"ç”µåŠ›çº¿æ®µæ•°: {len(powerline_segments)}")
        print(f"æ‹Ÿåˆæ›²çº¿æ•°: {len(all_fitted_curves)}")
        print(f"æ‹Ÿåˆæ—¶é—´: {fit_time:.2f}ç§’")
        print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
        
        success_rate = 0
        avg_inlier_ratio = 0
        avg_rmse = 0
        
        if powerline_segments:
            success_rate = len([info for info in all_fit_infos if info['fitted_segments'] > 0]) / len(powerline_segments) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        if all_fitted_curves:
            avg_inlier_ratio = np.mean([curve['inlier_ratio'] for curve in all_fitted_curves])
            avg_rmse = np.mean([curve['rmse'] for curve in all_fitted_curves])
            print(f"å¹³å‡å†…ç‚¹æ¯”ä¾‹: {avg_inlier_ratio:.2f}")
            print(f"å¹³å‡RMSE: {avg_rmse:.3f}ç±³")
        
        if all_fit_infos:
            total_segments = sum(info['fitted_segments'] for info in all_fit_infos)
            print(f"æ€»æ‹Ÿåˆæ®µæ•°: {total_segments}")
        
        # æ˜¾ç¤ºæ¯ä¸ªæ®µçš„ç»Ÿè®¡
        for i, info in enumerate(all_fit_infos):
            print(f"  æ®µ{info['segment_id']}: {info['total_points']}ç‚¹ â†’ {info['fitted_segments']}æ®µ "
                  f"(é•¿åº¦{info['total_length']:.1f}m, åˆ†ç±»{info['classification']})")
        
        # 5. å¯è§†åŒ–
        if visualize and all_fitted_curves:
            try:
                self.visualize_results(points, powerline_segments, all_fitted_curves)
            except Exception as e:
                print(f"å¯è§†åŒ–å¤±è´¥: {e}")
        
        # 6. å¯¼å‡ºç»“æœ
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
            # æ„å»ºåŸºæœ¬æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            base_filename = "powerline_curves"
            
            # å¦‚æœå¯ç”¨è‡ªåŠ¨æ·»åŠ åºå·ï¼Œè·å–å¯ç”¨çš„æ–‡ä»¶å
            if auto_increment:
                json_file = get_available_filename(os.path.join(output_dir, f"{base_filename}.json"))
                base_filename = os.path.splitext(os.path.basename(json_file))[0]
            else:
                json_file = os.path.join(output_dir, f"{base_filename}.json")
            
            # å¯¼å‡ºJSONå’Œå…¶ä»–ç»“æœ
            if save_json:
                # ä¿å­˜JSONæ–‡ä»¶
                self.export_json(all_fitted_curves, all_fit_infos, output_dir, f"{base_filename}.json")
                
                # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
                report_file = os.path.join(output_dir, f"{base_filename}_report.txt")
                self.export_report(powerline_segments, all_fitted_curves, all_fit_infos, report_file)
                
                # å¦‚æœéœ€è¦ï¼Œä¿å­˜XYZæ–‡ä»¶
                if save_xyz:
                    # åˆ›å»ºXYZç›®å½•
                    xyz_dir = os.path.join(output_dir, f"{base_filename}_curves")
                    os.makedirs(xyz_dir, exist_ok=True)
                    
                    # å¯¼å‡ºXYZæ–‡ä»¶
                    self.export_xyz_files(all_fitted_curves, xyz_dir)
            
            # å¯¼å‡ºHTMLæŠ¥å‘Š
            if save_html:
                html_file = os.path.join(output_dir, f"{base_filename}_report.html")
                self.export_html_report(
                    all_fitted_curves, 
                    all_fit_infos, 
                    html_file, 
                    total_points=len(points),
                    total_segments=len(powerline_segments),
                    success_rate=success_rate,
                    avg_inlier_ratio=avg_inlier_ratio,
                    avg_rmse=avg_rmse,
                    total_time=total_time,
                    fit_time=fit_time
                )
        
        return points, powerline_segments, all_fitted_curves, all_fit_infos
    
    def visualize_results(self, points, segments, fitted_curves):
        """å¯è§†åŒ–ç»“æœ"""
        print("å¼€å§‹å¯è§†åŒ–...")
        
        # åŸå§‹ç‚¹äº‘ï¼ˆç°è‰²ï¼‰
        pcd_original = o3d.geometry.PointCloud()
        pcd_original.points = o3d.utility.Vector3dVector(points)
        pcd_original.paint_uniform_color([0.7, 0.7, 0.7])
        
        # ç”µåŠ›çº¿æ®µï¼ˆä¸åŒé¢œè‰²ï¼‰
        pcd_segments = []
        colors = plt.cm.tab10(np.linspace(0, 1, 10))  # åªä½¿ç”¨10ç§é¢œè‰²
        
        for i, segment in enumerate(segments):
            pcd_segment = o3d.geometry.PointCloud()
            pcd_segment.points = o3d.utility.Vector3dVector(segment['points'])
            pcd_segment.paint_uniform_color(colors[i % 10][:3])
            pcd_segments.append(pcd_segment)
        
        # æ‹Ÿåˆæ›²çº¿ï¼ˆçº¢è‰²ç²—çº¿ï¼‰
        line_sets = []
        for i, curve_info in enumerate(fitted_curves):
            curve_points = curve_info['points']
            
            line_set = o3d.geometry.LineSet()
            line_set.points = o3d.utility.Vector3dVector(curve_points)
            lines = [[j, j+1] for j in range(len(curve_points)-1)]
            line_set.lines = o3d.utility.Vector2iVector(lines)
            
            # æ ¹æ®åˆ†ç±»ä½¿ç”¨ä¸åŒé¢œè‰²
            cls = curve_info['classification'] % 10  # å–æ¨¡ç¡®ä¿åœ¨é¢œè‰²èŒƒå›´å†…
            color = colors[cls][:3]
            colors_lines = [color for _ in range(len(lines))]
            line_set.colors = o3d.utility.Vector3dVector(colors_lines)
            line_sets.append(line_set)
        
        # æ˜¾ç¤º
        geometries = [pcd_original] + pcd_segments + line_sets
        o3d.visualization.draw_geometries(geometries)
    
    def export_json(self, fitted_curves, fit_infos, output_dir, json_filename="powerline_curves.json"):
        """å¯¼å‡ºJSONæ ¼å¼çš„æ‹Ÿåˆç»“æœï¼Œæ”¯æŒå½’ä¸€åŒ–åæ ‡"""
        print("å¯¼å‡ºJSONç»“æœ...")
        
        # åˆ›å»ºJSONæ•°æ®ç»“æ„
        json_data = {
            "metadata": {
                "generator": "RANSAC Power Line Fitting",
                "version": "2.0",
                "date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "totalSegments": len(fit_infos),
                "totalCurves": len(fitted_curves),
                "coordinateSystem": "normalized" if self.normalization_params else "original"
            },
            "parameters": {
                "segmentLength": self.segment_length,
                "overlapRatio": self.overlap_ratio,
                "ransacThreshold": self.ransac_threshold,
                "minSamplesRatio": self.min_samples_ratio,
                "polynomialDegree": self.polynomial_degree
            },
            "segments": [],
            "curves": []
        }
        
        # å¦‚æœæœ‰å½’ä¸€åŒ–å‚æ•°ï¼Œæ·»åŠ åˆ°metadataä¸­
        if self.normalization_params:
            json_data["normalization"] = {
                "source": "threejs_params",
                "applied": True,
                "originalBounds": self.normalization_params.get("original_bounds"),
                "center": self.normalization_params.get("center"),
                "scale": self.normalization_params["normalization"]["scale"],
                "translate": self.normalization_params["normalization"]["translate"]
            }
        
        # æ·»åŠ æ®µä¿¡æ¯
        for info in fit_infos:
            segment_info = {
                "segmentId": info["segment_id"],
                "classification": int(info["classification"]),
                "totalPoints": int(info["total_points"]),
                "fittedSegments": int(info["fitted_segments"]),
                "totalLength": float(info["total_length"])
            }
            json_data["segments"].append(segment_info)
        
        # æ·»åŠ æ›²çº¿ä¿¡æ¯
        for curve in fitted_curves:
            # è·å–åŸå§‹æ›²çº¿ç‚¹
            original_points = curve["points"]
            
            # åº”ç”¨å½’ä¸€åŒ–ï¼ˆå¦‚æœæœ‰å½’ä¸€åŒ–å‚æ•°ï¼‰
            if self.normalization_params:
                normalized_points = apply_normalization(original_points, self.normalization_params)
                curve_points_to_export = normalized_points
                start_point_to_export = apply_normalization(np.array([curve["start_point"]]), self.normalization_params)[0].tolist()
                end_point_to_export = apply_normalization(np.array([curve["end_point"]]), self.normalization_params)[0].tolist()
            else:
                curve_points_to_export = original_points
                start_point_to_export = curve["start_point"]
                end_point_to_export = curve["end_point"]
            
            # ç®€åŒ–æ›²çº¿ç‚¹ä»¥å‡å°JSONå¤§å° - åªä¿ç•™10ä¸ªç‚¹
            simplified_points = []
            if len(curve_points_to_export) > 0:
                step = max(1, len(curve_points_to_export) // 10)
                for i in range(0, len(curve_points_to_export), step):
                    simplified_points.append(curve_points_to_export[i].tolist())
                
                # ç¡®ä¿è‡³å°‘åŒ…å«èµ·ç‚¹å’Œç»ˆç‚¹
                if len(simplified_points) < 2 and len(curve_points_to_export) >= 2:
                    simplified_points = [curve_points_to_export[0].tolist(), curve_points_to_export[-1].tolist()]
            
            curve_info = {
                "segmentId": curve["segment_id"],
                "subsegmentId": curve["subsegment_id"],
                "classification": int(curve["classification"]),
                "inlierRatio": float(curve["inlier_ratio"]),
                "rmse": float(curve["rmse"]),
                "coefficients": curve["coefs"],
                "startPoint": start_point_to_export,
                "endPoint": end_point_to_export,
                "points": simplified_points,
                "originalPointsCount": curve.get("original_points_count", 0)
            }
            
            # å¦‚æœæœ‰å½’ä¸€åŒ–å‚æ•°ï¼Œä¹Ÿä¿å­˜åŸå§‹åæ ‡ï¼ˆå¯é€‰ï¼‰
            if self.normalization_params:
                # ç®€åŒ–åŸå§‹åæ ‡ç‚¹
                original_simplified = []
                if len(original_points) > 0:
                    step = max(1, len(original_points) // 10)
                    for i in range(0, len(original_points), step):
                        original_simplified.append(original_points[i].tolist())
                    
                    if len(original_simplified) < 2 and len(original_points) >= 2:
                        original_simplified = [original_points[0].tolist(), original_points[-1].tolist()]
                
                curve_info["originalCoordinates"] = {
                    "startPoint": curve["start_point"],
                    "endPoint": curve["end_point"],
                    "points": original_simplified
                }
            
            json_data["curves"].append(curve_info)
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_file = os.path.join(output_dir, json_filename)
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2)
        
        print(f"JSONç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        if self.normalization_params:
            print("  åæ ‡å·²å½’ä¸€åŒ–ï¼Œé€‚åˆç›´æ¥åœ¨Three.jsä¸­ä½¿ç”¨")
        else:
            print("  ä½¿ç”¨åŸå§‹åæ ‡ç³»")
    
    def export_report(self, segments, fitted_curves, fit_infos, report_file):
        """ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶"""
        print(f"ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶: {report_file}")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("åŸºäºRANSACçš„ç”µåŠ›çº¿é‡å»ºæŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ç”µåŠ›çº¿æ®µæ•°: {len(segments)}\n")
            f.write(f"æ‹Ÿåˆæ›²çº¿æ•°: {len(fitted_curves)}\n")
            f.write(f"åæ ‡ç³»ç»Ÿ: {'å½’ä¸€åŒ–' if self.normalization_params else 'åŸå§‹'}\n\n")
            
            if self.normalization_params:
                f.write("å½’ä¸€åŒ–å‚æ•°:\n")
                f.write(f"  ç¼©æ”¾å› å­: {self.normalization_params['normalization']['scale']:.6f}\n")
                translate = self.normalization_params['normalization']['translate']
                f.write(f"  å¹³ç§»é‡: ({translate['x']:.2f}, {translate['y']:.2f}, {translate['z']:.2f})\n\n")
            
            f.write("å‚æ•°è®¾ç½®:\n")
            f.write(f"  åˆ†æ®µé•¿åº¦: {self.segment_length}ç±³\n")
            f.write(f"  é‡å æ¯”ä¾‹: {self.overlap_ratio}\n")
            f.write(f"  RANSACé˜ˆå€¼: {self.ransac_threshold}ç±³\n")
            f.write(f"  å¤šé¡¹å¼åº¦æ•°: {self.polynomial_degree}\n\n")
            
            for info in fit_infos:
                f.write(f"ç”µåŠ›çº¿æ®µ {info['segment_id']} (åˆ†ç±» {info['classification']}):\n")
                f.write(f"  ç‚¹æ•°: {info['total_points']}\n")
                f.write(f"  æ‹Ÿåˆæ®µæ•°: {info['fitted_segments']}\n")
                f.write(f"  æ€»é•¿åº¦: {info['total_length']:.2f}ç±³\n\n")
            
            f.write("=" * 80 + "\n")
            f.write("æŠ¥å‘Šç»“æŸ\n")
    
    def export_xyz_files(self, fitted_curves, xyz_dir):
        """å¯¼å‡ºXYZæ ¼å¼çš„æ›²çº¿æ–‡ä»¶ï¼Œæ”¯æŒå½’ä¸€åŒ–åæ ‡"""
        print(f"å¯¼å‡ºXYZæ›²çº¿æ–‡ä»¶åˆ°: {xyz_dir}")
        
        for i, curve_info in enumerate(fitted_curves):
            original_points = curve_info['points']
            segment_id = curve_info['segment_id']
            subsegment_id = curve_info['subsegment_id']
            cls = curve_info['classification']
            
            # åº”ç”¨å½’ä¸€åŒ–ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.normalization_params:
                curve_points = apply_normalization(original_points, self.normalization_params)
                coord_suffix = "_normalized"
            else:
                curve_points = original_points
                coord_suffix = "_original"
            
            # ä¿å­˜ä¸ºXYZæ–‡ä»¶
            curve_file = os.path.join(xyz_dir, f"powerline_seg{segment_id}_sub{subsegment_id}_class{cls}{coord_suffix}.xyz")
            with open(curve_file, 'w') as f:
                f.write(f"# RANSACæ‹Ÿåˆçš„ç”µåŠ›çº¿æ›²çº¿ - æ®µ{segment_id} å­æ®µ{subsegment_id} åˆ†ç±»{cls}\n")
                f.write(f"# åæ ‡ç³»ç»Ÿ: {'å½’ä¸€åŒ–' if self.normalization_params else 'åŸå§‹'}\n")
                f.write("# X Y Z\n")
                for point in curve_points:
                    f.write(f"{point[0]:.6f} {point[1]:.6f} {point[2]:.6f}\n")
    
    def export_html_report(self, fitted_curves, fit_infos, html_file, total_points=0, total_segments=0, 
                          success_rate=0, avg_inlier_ratio=0, avg_rmse=0, total_time=0, fit_time=0):
        """å¯¼å‡ºHTMLæ ¼å¼çš„æŠ¥å‘Šï¼Œç±»ä¼¼äºransac_report.htmlçš„æ ·å¼"""
        print(f"ç”ŸæˆHTMLæŠ¥å‘Š: {html_file}")
        
        # åˆ›å»ºHTMLæŠ¥å‘Š
        html_content = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RANSACç”µåŠ›çº¿é‡å»ºæŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }}
        .summary {{ background: #ecf0f1; padding: 15px; margin: 20px 0; border-radius: 5px; }}
        .curve-item {{ background: #f8f9fa; margin: 10px 0; padding: 15px; border-left: 4px solid #3498db; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}
        .stat-card {{ background: white; padding: 15px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .good {{ color: #27ae60; }} .warning {{ color: #f39c12; }} .error {{ color: #e74c3c; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ RANSACç”µåŠ›çº¿é‡å»ºæŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š å¤„ç†æ¦‚è¦</h2>
        <div class="stats">
            <div class="stat-card">
                <h3>ğŸ“ è¾“å…¥æ•°æ®</h3>
                <p>æ€»ç‚¹æ•°: {total_points:,}</p>
                <p>ç”µåŠ›çº¿æ®µæ•°: {total_segments}</p>
            </div>
            <div class="stat-card">
                <h3>ğŸ“ˆ è¾“å‡ºç»“æœ</h3>
                <p>æ‹Ÿåˆæ›²çº¿æ•°: {len(fitted_curves)}</p>
                <p>æˆåŠŸç‡: {success_rate:.1f}%</p>
            </div>
            <div class="stat-card">
                <h3>â±ï¸ å¤„ç†æ—¶é—´</h3>
                <p>æ€»æ—¶é—´: {total_time:.2f}ç§’</p>
                <p>æ‹Ÿåˆæ—¶é—´: {fit_time:.2f}ç§’</p>
            </div>
            <div class="stat-card">
                <h3>ğŸ¯ è´¨é‡æŒ‡æ ‡</h3>
                <p>å¹³å‡RMSE: {avg_rmse:.3f}ç±³</p>
                <p>å¹³å‡å†…ç‚¹æ¯”: {avg_inlier_ratio:.2f}</p>
            </div>
        </div>
    </div>
    
    <h2>ğŸ“‹ æ‹Ÿåˆæ›²çº¿è¯¦æƒ…</h2>
    <table>
        <thead>
            <tr>
                <th>ID</th>
                <th>ç‚¹æ•°</th>
                <th>é•¿åº¦(m)</th>
                <th>RMSE(m)</th>
                <th>å†…ç‚¹æ¯”ä¾‹</th>
                <th>è´¨é‡è¯„çº§</th>
            </tr>
        </thead>
        <tbody>
"""
        
        # æ·»åŠ æ¯æ¡æ‹Ÿåˆæ›²çº¿çš„ä¿¡æ¯
        curve_id = 1
        for curve in fitted_curves:
            segment_id = curve['segment_id']
            subsegment_id = curve['subsegment_id']
            
            # è®¡ç®—æ›²çº¿é•¿åº¦ - èµ·ç‚¹å’Œç»ˆç‚¹ä¹‹é—´çš„æ¬§æ°è·ç¦»
            start = np.array(curve['start_point'])
            end = np.array(curve['end_point'])
            length = np.linalg.norm(end - start)
            
            # ç¡®å®šè´¨é‡ç­‰çº§
            if curve['rmse'] < 0.3 and curve['inlier_ratio'] > 0.7:
                quality = '<span class="good">ä¼˜ç§€</span>'
            elif curve['rmse'] < 0.5 and curve['inlier_ratio'] > 0.5:
                quality = '<span class="warning">è‰¯å¥½</span>'
            else:
                quality = '<span class="error">ä¸€èˆ¬</span>'
            
            # æ ¼å¼åŒ–ID
            curve_id_str = f"powerline_{segment_id:03d}_{subsegment_id}"
            
            # æ·»åŠ è¡Œ
            html_content += f"""
            <tr>
                <td>{curve_id_str}</td>
                <td>{curve.get('original_points_count', 0)}</td>
                <td>{length:.2f}</td>
                <td>{curve['rmse']:.3f}</td>
                <td>{curve['inlier_ratio']:.2f}</td>
                <td>{quality}</td>
            </tr>
"""
            curve_id += 1
        
        # å®Œæˆè¡¨æ ¼å¹¶æ·»åŠ å‚æ•°ä¿¡æ¯
        html_content += f"""
        </tbody>
    </table>
    
    <h2>ğŸ”§ ç®—æ³•å‚æ•°</h2>
    <div class="summary">
        <p><strong>åˆ†æ®µé•¿åº¦:</strong> {self.segment_length:.1f}ç±³</p>
        <p><strong>é‡å æ¯”ä¾‹:</strong> {self.overlap_ratio * 100:.0f}%</p>
        <p><strong>RANSACé˜ˆå€¼:</strong> {self.ransac_threshold:.1f}ç±³</p>
        <p><strong>æœ€å°æ ·æœ¬æ¯”ä¾‹:</strong> {self.min_samples_ratio * 100:.0f}%</p>
        <p><strong>æ‹Ÿåˆç±»å‹:</strong> {'çº¿æ€§æ‹Ÿåˆ' if self.polynomial_degree == 1 else f'å¤šé¡¹å¼æ‹Ÿåˆ(åº¦æ•°: {self.polynomial_degree})'}</p>
    </div>
    
    <footer style="margin-top: 40px; text-align: center; color: #7f8c8d;">
        <p>æŠ¥å‘Šç”±RANSACç”µåŠ›çº¿é‡å»ºç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ</p>
    </footer>
</body>
</html>
"""
        
        # å†™å…¥HTMLæ–‡ä»¶
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"HTMLæŠ¥å‘Šå·²ä¿å­˜åˆ°: {html_file}")


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="åŸºäºRANSACçš„ç”µåŠ›çº¿æ›²çº¿æ‹Ÿåˆï¼Œè¾“å‡ºä¸ºJSONæ ¼å¼ï¼Œæ”¯æŒå½’ä¸€åŒ–åæ ‡")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥LASæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", "-o", required=True, help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--normalization_file", "-nf", help="å½’ä¸€åŒ–å‚æ•°JSONæ–‡ä»¶è·¯å¾„(å¯é€‰)")
    parser.add_argument("--segment_length", "-sl", type=float, default=60.0, help="åˆ†æ®µé•¿åº¦(ç±³)")
    parser.add_argument("--overlap_ratio", "-or", type=float, default=0.1, help="é‡å æ¯”ä¾‹")
    parser.add_argument("--ransac_threshold", "-rt", type=float, default=1.2, help="RANSACé˜ˆå€¼(ç±³)")
    parser.add_argument("--min_samples_ratio", "-msr", type=float, default=0.5, help="æœ€å°æ ·æœ¬æ¯”ä¾‹")
    parser.add_argument("--visualize", "-v", action="store_true", help="æ˜¾ç¤ºå¯è§†åŒ–ç»“æœ")
    parser.add_argument("--save_xyz", "-xyz", action="store_true", help="ä¿å­˜XYZæ–‡ä»¶")
    parser.add_argument("--no_html", "-nh", action="store_true", help="ä¸ç”ŸæˆHTMLæŠ¥å‘Š")
    parser.add_argument("--no_auto_increment", "-nai", action="store_true", help="ç¦ç”¨è‡ªåŠ¨æ–‡ä»¶ç¼–å·")
    
    args = parser.parse_args()
    
    print("åˆå§‹åŒ–RANSACç”µåŠ›çº¿é‡å»ºå™¨...")
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºå¤„ç†å™¨
    processor = RANSACPowerLineFitting(
        segment_length=args.segment_length,
        overlap_ratio=args.overlap_ratio,
        ransac_threshold=args.ransac_threshold,
        min_samples_ratio=args.min_samples_ratio,
        polynomial_degree=1,  # å›ºå®šä½¿ç”¨çº¿æ€§æ‹Ÿåˆ
        normalization_file=args.normalization_file
    )
    
    try:
        print("="*80)
        print("å¼€å§‹RANSACç”µåŠ›çº¿é‡å»º")
        print("ä¼˜åŠ¿:")
        print("1. æ— éœ€å¤æ‚çš„å‚æ•°è°ƒä¼˜")
        print("2. è‡ªåŠ¨å¤„ç†å™ªå£°ç‚¹")
        print("3. é€‚åº”å„ç§ç”µåŠ›çº¿å½¢çŠ¶")
        print("4. åˆ†æ®µå¤„ç†ï¼Œé¿å…è¿‡æ‹Ÿåˆ")
        print("5. ç»“æœç¨³å®šå¯é ")
        print("6. æ”¯æŒå½’ä¸€åŒ–åæ ‡è¾“å‡ºï¼ˆThree.jså‹å¥½ï¼‰")
        print("7. ç”Ÿæˆå¯è§†åŒ–HTMLæŠ¥å‘Š")
        print("="*80)
        
        # å¤„ç†
        results = processor.process_powerline_cloud(
            args.input, 
            args.output_dir,
            save_json=True,
            visualize=args.visualize,
            save_xyz=args.save_xyz,
            save_html=not args.no_html,
            auto_increment=not args.no_auto_increment
        )
        
        print("\nRANSACç”µåŠ›çº¿é‡å»ºå®Œæˆï¼")
        
        if args.normalization_file:
            print("æç¤º: è¾“å‡ºçš„JSONæ–‡ä»¶åŒ…å«å½’ä¸€åŒ–åæ ‡ï¼Œå¯ç›´æ¥ç”¨äºThree.jsæ¸²æŸ“")
        
        if not args.no_html:
            print("æç¤º: HTMLæŠ¥å‘Šå·²ç”Ÿæˆï¼Œå¯åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ç”µåŠ›çº¿æ‹Ÿåˆç»“æœ")
        
    except Exception as e:
        print(f"å¤„ç†é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    start_time = time.time()
    exit_code = main()
    end_time = time.time()
    print(f"\næ€»è¿è¡Œæ—¶é—´: {end_time - start_time:.2f} ç§’")
    exit(exit_code)